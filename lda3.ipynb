{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from googletrans import Translator\n",
    "from langdetect import detect, DetectorFactory\n",
    "import numpy as np\n",
    "import time\n",
    "import ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NoDuplicates_Translated2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse conversations\n",
    "text_list = []\n",
    "text_arrays = []\n",
    "for conversation in df[\"vectorized_col_1\"]:\n",
    "    text = \"\"\n",
    "    text_arr = []\n",
    "\n",
    "    # parsed_conversation = ast.literal_eval(conversation)\n",
    "    conversation = conversation[2:-1]\n",
    "\n",
    "    values = re.split(r\"(?=\\{'from': 'human',|\\{'from': 'gpt',)\", conversation, flags=re.IGNORECASE)\n",
    "    values = values[1:]\n",
    "    for value in values:\n",
    "        \n",
    "        value = re.sub(r\"{'from': 'human', 'value': \", \"\", value, flags=re.IGNORECASE)\n",
    "        value = re.sub(r\"{'from': 'gpt', 'value': \", \"\", value, flags=re.IGNORECASE)\n",
    "\n",
    "        \n",
    "        text += value\n",
    "        text_arr.append(value)\n",
    "\n",
    "    text_list.append(text)\n",
    "    text_arrays.append(text_arr)\n",
    "\n",
    "df[\"text\"] = text_list\n",
    "df[\"values\"] = text_arrays\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    # remove code\n",
    "    a=re.compile(\"<code>.*?</code>\")\n",
    "    text = a.sub('', text)\n",
    "    # remove angle brackets and all characters inside angle bracket\n",
    "    a=re.compile(\"<.*?>\")\n",
    "    text = a.sub('', text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    tokens = [word for word in tokens if word not in cached]\n",
    "    return tokens\n",
    "\n",
    "# preprocess all text\n",
    "# preprocessed_text = [' '.join(preprocess_text(document)) for document in corpus]\n",
    "\n",
    "preprocessed_text = []\n",
    "i = 0\n",
    "for document in corpus:\n",
    "    print(i)\n",
    "    preprocessed_document = ' '.join(preprocess_text(document))\n",
    "    preprocessed_text.append(preprocessed_document)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_text\"] = preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"preprocessed_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents\n",
    "tokenized_documents = [doc.split() for doc in corpus]\n",
    "\n",
    "# create a dictionary representation of the documents\n",
    "dictionary = Dictionary(tokenized_documents)\n",
    "\n",
    "# convert document into bag-of-words format\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus, num_topics=100, id2word=dictionary, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_topics=100)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup: get the model's topics in their native ordering...\n",
    "all_topics = lda_model.print_topics(num_topics=100)\n",
    "# ...then create a empty list per topic to collect the docs:\n",
    "docs_per_topic = [[] for _ in all_topics]\n",
    "\n",
    "# now, for every doc...\n",
    "for doc_id, doc_bow in enumerate(corpus):\n",
    "    # ...get its topics...\n",
    "    doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "    # ...& for each of its topics...\n",
    "    for topic_id, score in doc_topics:\n",
    "        # ...add the doc_id & its score to the topic's doc list\n",
    "        docs_per_topic[topic_id].append((doc_id, score))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
