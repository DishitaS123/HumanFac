{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataframe_Wrangled_NoDuplicates.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse conversations\n",
    "text_list = []\n",
    "text_arrays = []\n",
    "for conversation in df[\"conversations\"]:\n",
    "    text = \"\"\n",
    "    text_arr = []\n",
    "    parsed_conversation = eval(conversation)\n",
    "    for dialogue in parsed_conversation:\n",
    "        text += dialogue[\"value\"] + \" \"\n",
    "        text_arr.append(dialogue[\"value\"])\n",
    "    text_list.append(text)\n",
    "    text_arrays.append(text_arr)\n",
    "\n",
    "# create column with the conversation text\n",
    "df[\"text\"] = text_list\n",
    "df[\"values\"] = text_arrays\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine language of each conversation\n",
    "languages = []\n",
    "for value in df[\"values\"]:\n",
    "    try:\n",
    "        lang = detect(value[0])\n",
    "        languages.append(lang)\n",
    "    except:\n",
    "        try:\n",
    "            lang = detect(value[1])\n",
    "            languages.append(lang)\n",
    "        except:\n",
    "            print(\"error2\")\n",
    "            print(value[1])\n",
    "\n",
    "\n",
    "df[\"lang\"] = languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortened = df[['id', 'values', 'lang']]\n",
    "df_shortened.to_csv('file_name.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "en       18345\n",
       "ko         822\n",
       "zh-cn      624\n",
       "fr         522\n",
       "es         464\n",
       "ja         238\n",
       "ca         198\n",
       "it         177\n",
       "vi         163\n",
       "pt         162\n",
       "de         148\n",
       "nl         133\n",
       "no         133\n",
       "ru         124\n",
       "ro         121\n",
       "sw          86\n",
       "et          76\n",
       "zh-tw       75\n",
       "id          63\n",
       "af          63\n",
       "cy          62\n",
       "fi          62\n",
       "pl          56\n",
       "da          54\n",
       "tl          40\n",
       "sv          33\n",
       "hr          32\n",
       "so          30\n",
       "tr          29\n",
       "sk          21\n",
       "sl          21\n",
       "uk          18\n",
       "th          16\n",
       "cs          11\n",
       "lt          11\n",
       "bg          10\n",
       "he           9\n",
       "hu           8\n",
       "mk           6\n",
       "sq           3\n",
       "fa           3\n",
       "lv           2\n",
       "el           2\n",
       "ar           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "async def translate(value):\n",
    "      result = await translator.translate(value, dest=\"en\")\n",
    "      return result\n",
    "\n",
    "translated_values = []\n",
    "for index, item in df[\"values\"].items():\n",
    "    if df[\"lang\"][index] != \"en\":\n",
    "        translated_conversation = []\n",
    "        for element in item:\n",
    "            a=re.compile(\"<.*?>\")\n",
    "            element = a.sub('', element)\n",
    "            if len(element) > 15000:\n",
    "                chunks = []\n",
    "                for i in range(0, len(element), 15000):\n",
    "                    chunks.append(element[i:i + 15000])\n",
    "                \n",
    "                translated_chunks = []\n",
    "                for chunk in chunks:\n",
    "                    result = await translate(chunk)\n",
    "                    translated_chunks.append(result.text)\n",
    "                \n",
    "                result = \" \".join(translated_chunks)\n",
    "                translated_conversation.append(result)\n",
    "\n",
    "            else:\n",
    "                result = await translate(element)\n",
    "                translated_conversation.append(result.text)\n",
    "                \n",
    "        translated_values.append(translated_conversation)\n",
    "    else:\n",
    "        translated_values.append(item)\n",
    "\n",
    "df[\"translated_values\"] = translated_values\n",
    "\n",
    "\n",
    "# 67 conversations exceed 15,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    # remove angle brackets and all characters inside angle bracket\n",
    "    # a=re.compile(\"<.*?>\")\n",
    "    # text = a.sub('', text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords\n",
    "    # tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# preprocess all text\n",
    "preprocessed_text = [' '.join(preprocess_text(document)) for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessed_text\"] = preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"preprocessed_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents\n",
    "tokenized_documents = [doc.split() for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary representation of the documents\n",
    "dictionary = Dictionary(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert document into bag-of-words format\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
